"""
Single-Agent Evaluation Experiment (Qwen3-4B SFT-GRPO-DPO via vLLM)
===================================================================

This script evaluates a single-agent pipeline for teaching pack generation using three metrics:
1. Content Accuracy (Acc) - Factual correctness of slides and quiz
2. Exact Match / Semantic Coverage (EM) - Coverage of key concepts from lesson summary
3. Educational Soundness (ES) - Pedagogical appropriateness

Generation: Qwen3-4B with SFT-GRPO-DPO LoRA adapter (vLLM)
Evaluation: Gemini (configurable)

Usage:
    python experiments/single_agent_evaluation_experiment_vllm_qwen3_grpo_dpo.py --lesson_summary <path> --ground_truth <path>
"""

import os
import sys
import json
import asyncio
import argparse
import re
from datetime import datetime
from typing import Dict, List, Any, Optional
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# Import Single-Agent pipeline components
from src.llm.base import AgentClient
from src.utils.config_loader import load_config, resolve_value
from src.utils.reproducibility import set_seed
from src.models.teaching_pack_models import (
    LessonSummary,
    SkillSet,
    Diagnostic,
    GroupProfile,
    PackPlan,
    Slides,
    Video,
    Quiz,
)
from src.utils.workflow_helpers import export_final_results
from src.utils.basetools.pdf_parser import extract_text_from_pdf

# Import Google Gemini for evaluation
from pydantic_ai import Agent as PydanticAgent
from pydantic_ai.models.google import GoogleModel
from pydantic_ai.providers.google import GoogleProvider
from pydantic_ai.models.openai import OpenAIChatModel
from pydantic_ai.providers.openai import OpenAIProvider
from pydantic_ai.exceptions import ModelHTTPError
from pydantic import BaseModel


# =====================================================
# DEFAULTS
# =====================================================

DEFAULT_VLLM_MODEL = "Qwen/Qwen3-4B"
DEFAULT_VLLM_LORA = "qwen3-grpo-dpo"
DEFAULT_GEMINI_MODEL = "gemini-2.0-flash"


# =====================================================
# EVALUATION RESULT SCHEMAS
# =====================================================

class UnitScore(BaseModel):
    """Score for a single unit (slide or quiz question)"""
    unit_id: str
    unit_type: str  # "slide" or "quiz"
    accuracy: float  # 0, 0.5, or 1
    explanation: str


class ConceptCoverage(BaseModel):
    """Coverage score for a single concept"""
    concept: str
    coverage: float  # 0, 0.5, or 1
    explanation: str


class EducationalSoundnessScore(BaseModel):
    """Educational soundness evaluation"""
    criterion: str
    score: float  # 0 to 1
    explanation: str


class EvaluationResult(BaseModel):
    """Complete evaluation result with all metrics"""
    # Content Accuracy
    accuracy_scores: List[UnitScore]
    accuracy_total: float

    # Exact Match / Semantic Coverage
    concept_coverage: List[ConceptCoverage]
    coverage_total: float

    # Educational Soundness
    educational_soundness: List[EducationalSoundnessScore]
    educational_soundness_total: float

    # Overall Score
    overall_score: float

    # Metadata
    num_slides: int
    num_quiz_questions: int
    num_concepts: int
    evaluation_timestamp: str


# =====================================================
# ENV LOADER
# =====================================================

def _load_env_var_from_file(var_name: str, env_path: str = ".env") -> None:
    """Load a single env var from a .env file if not already set."""
    if os.getenv(var_name):
        return
    if not os.path.exists(env_path):
        return
    try:
        with open(env_path, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line or line.startswith("#"):
                    continue
                if line.startswith(var_name + "="):
                    value = line.split("=", 1)[1].strip().strip('"').strip("'")
                    if value:
                        os.environ[var_name] = value
                    return
    except Exception:
        # Fail silently; caller will validate env vars later
        return


# =====================================================
# SINGLE-AGENT PIPELINE SCHEMAS
# =====================================================

class TeachingPackBundle(BaseModel):
    """Container for a full teaching pack per group."""
    group: GroupProfile
    pack_plan: PackPlan
    slides: Slides
    video: Video
    quiz: Quiz


class SingleAgentPipelineOutput(BaseModel):
    """Complete pipeline output generated by a single agent."""
    lesson_summary: LessonSummary
    skill_set: SkillSet
    diagnostic: Diagnostic
    groups: List[GroupProfile]
    teaching_packs: List[TeachingPackBundle]


SINGLE_AGENT_PIPELINE_PROMPT = """You are a concise instructional designer.

Task: In ONE response, produce JSON that matches SingleAgentPipelineOutput.

Input provides lesson_summary or lesson_text and num_groups.

Rules (keep output VERY SHORT):
- If lesson_summary is provided, copy it as-is (set lesson_content to "" if long).
- If lesson_text is provided, derive lesson_summary strictly from it.
- Use Vietnamese; keep all strings <= 6 words.
- Allowed enums only: mastery_level {low,medium,high,advanced}, learning_pace {slow,moderate,fast}, difficulty {easy,medium,hard}.
- groups length == num_groups; group_id "group_1"...; students may be [] to save space.
- teaching_packs length == num_groups and order matches groups.
- pack_plan.group_id == group.group_id.
- Diagnostic: EXACTLY 3 questions; skills_covered matches skill_ids.
- Slides: 3-4 slides; slide_id "slide_1"...; generated_url null.
- Video: generated_url and thumbnail_url null; script/visual_description can be "".
- Quiz: EXACTLY 3 questions; practice_exercises []; answer_key {}; estimated_time integer.
- Keep definitions {} and examples [] if not needed.

Return JSON only, no extra text.
"""


# =====================================================
# SINGLE-AGENT PIPELINE RUNNER
# =====================================================

def _normalize_lesson_summary(raw: Dict[str, Any]) -> LessonSummary:
    """Coerce ground-truth lesson summary into the expected schema."""
    data = dict(raw)
    data["grade"] = str(data.get("grade", ""))
    if "key_concepts" in data:
        data["key_concepts"] = [str(x) for x in (data.get("key_concepts") or [])]
    if "examples" in data:
        data["examples"] = [str(x) for x in (data.get("examples") or [])]
    if "definitions" in data and isinstance(data["definitions"], list):
        defs: Dict[str, str] = {}
        for item in data["definitions"]:
            if isinstance(item, dict):
                term = item.get("term") or item.get("key") or item.get("name")
                definition = item.get("definition") or item.get("value") or item.get("desc")
                if term is not None and definition is not None:
                    defs[str(term)] = str(definition)
        data["definitions"] = defs
    data.setdefault("lesson_content", "")
    return LessonSummary(**data)


def _lesson_summary_from_text(text: str) -> Dict[str, Any]:
    text = (text or "").strip()
    title = text.split(".")[0].strip() if text else "Bi hc"
    if len(title) > 60:
        title = title[:60].rsplit(" ", 1)[0]
    return {
        "title": title or "Bi hc",
        "subject": "",
        "grade": "",
        "key_concepts": [],
        "definitions": {},
        "examples": [],
        "lesson_content": text[:800],
    }


def _strip_code_fence(text: str) -> str:
    cleaned = text.strip()
    if cleaned.startswith("```"):
        lines = cleaned.splitlines()
        if lines and lines[0].startswith("```"):
            lines = lines[1:]
        if lines and lines[-1].strip().startswith("```"):
            lines = lines[:-1]
        cleaned = "\n".join(lines).strip()
    return cleaned


def _compact_raw_text(text: str, max_chars: int = 1500) -> str:
    """Normalize whitespace and trim raw lesson text for small context windows."""
    cleaned = re.sub(r"\s+", " ", text or "").strip()
    if len(cleaned) <= max_chars:
        return cleaned
    trimmed = cleaned[:max_chars]
    if " " in trimmed:
        trimmed = trimmed.rsplit(" ", 1)[0]
    return trimmed + " ..."


def _extract_json_block(text: str) -> str:
    start = text.find("{")
    if start == -1:
        raise ValueError("No JSON object found in model output.")
    depth = 0
    for i in range(start, len(text)):
        ch = text[i]
        if ch == "{":
            depth += 1
        elif ch == "}":
            depth -= 1
            if depth == 0:
                return text[start : i + 1]
    raise ValueError("Unclosed JSON object in model output.")


def _extract_max_tokens_limit(err: Exception) -> int | None:
    message = ""
    if isinstance(err, ModelHTTPError):
        body = getattr(err, "body", None)
        if isinstance(body, dict):
            message = body.get("message") or body.get("error", {}).get("message") or ""
    if not message:
        message = str(err)

    match = re.search(r"maximum context length is (\d+)", message)
    match_input = re.search(r"request has (\d+) input tokens", message)
    if match and match_input:
        max_len = int(match.group(1))
        input_tokens = int(match_input.group(1))
        return max(16, max_len - input_tokens)

    match_alt = re.search(r"\((\d+) > (\d+) - (\d+)\)", message)
    if match_alt:
        max_len = int(match_alt.group(2))
        input_tokens = int(match_alt.group(3))
        return max(16, max_len - input_tokens)

    return None


def _normalize_skill_set_dict(data: Dict[str, Any]) -> Dict[str, Any]:
    skills = data.get("skills")
    if isinstance(skills, list):
        normalized = []
        for skill in skills:
            if not isinstance(skill, dict):
                continue
            fixed = dict(skill)
            if "skill_id" not in fixed:
                fixed["skill_id"] = fixed.get("id") or fixed.get("skillId")
            if "name" not in fixed:
                fixed["name"] = fixed.get("title") or fixed.get("label")
            if fixed.get("name") is None:
                fixed["name"] = "Unnamed skill"
            if "description" not in fixed:
                fixed["description"] = fixed.get("desc") or fixed.get("detail")
            if fixed.get("description") is None:
                fixed["description"] = ""
            if "is_prerequisite" not in fixed:
                fixed["is_prerequisite"] = bool(fixed.get("prerequisite", False))
            if "weight" not in fixed:
                fixed["weight"] = 0.7
            normalized.append(fixed)
        data["skills"] = normalized
    data.setdefault("skill_dependencies", {})
    return data


def _fallback_skill_set(lesson_summary: Dict[str, Any]) -> Dict[str, Any]:
    concepts = lesson_summary.get("key_concepts") or []
    skills = []
    if concepts:
        for idx, concept in enumerate(concepts[:3], start=1):
            skills.append(
                {
                    "skill_id": f"skill_{idx}",
                    "name": str(concept)[:40],
                    "description": "",
                    "weight": 0.7,
                    "is_prerequisite": False,
                }
            )
    if not skills:
        skills = [
            {
                "skill_id": "skill_1",
                "name": "Ky nang co ban",
                "description": "",
                "weight": 0.7,
                "is_prerequisite": False,
            }
        ]
    return {"skills": skills, "skill_dependencies": {}}


def _normalize_diagnostic_dict(data: Dict[str, Any]) -> Dict[str, Any]:
    if "diagnostic" in data and isinstance(data["diagnostic"], dict):
        inner = data.pop("diagnostic")
        for key in ("questions", "total_questions", "skills_covered"):
            if key not in data and key in inner:
                data[key] = inner[key]
    questions = data.get("questions")
    if isinstance(questions, list):
        normalized_qs = []
        for idx, q in enumerate(questions, start=1):
            if not isinstance(q, dict):
                continue
            fixed_q = dict(q)
            if "question_id" not in fixed_q:
                fixed_q["question_id"] = fixed_q.get("id") or f"q{idx}"
            if "question_text" not in fixed_q:
                fixed_q["question_text"] = fixed_q.get("question") or fixed_q.get("prompt")
            if "correct_answer" not in fixed_q:
                fixed_q["correct_answer"] = fixed_q.get("answer") or fixed_q.get("correct")
            if "options" not in fixed_q:
                fixed_q["options"] = fixed_q.get("choices") or []
            if not isinstance(fixed_q.get("options"), list):
                fixed_q["options"] = [str(fixed_q["options"])]
            diff = fixed_q.get("difficulty")
            diff_norm = str(diff).lower() if diff is not None else "medium"
            if diff_norm not in ("easy", "medium", "hard"):
                diff_norm = "medium"
            fixed_q["difficulty"] = diff_norm
            if fixed_q.get("question_text") is None:
                fixed_q["question_text"] = ""
            if fixed_q.get("correct_answer") is None:
                fixed_q["correct_answer"] = ""
            else:
                fixed_q["correct_answer"] = str(fixed_q["correct_answer"])
            if "skill_id" not in fixed_q:
                fixed_q["skill_id"] = ""
            if "rationale" not in fixed_q:
                fixed_q["rationale"] = ""
            normalized_qs.append(fixed_q)
        data["questions"] = normalized_qs
    data.setdefault("questions", [])
    data.setdefault("skills_covered", [])
    data.setdefault("total_questions", len(data.get("questions", [])))
    return data


def _fallback_diagnostic(skill_set: Dict[str, Any]) -> Dict[str, Any]:
    skills = skill_set.get("skills") or []
    questions = []
    for idx, skill in enumerate(skills[:3], start=1):
        skill_id = skill.get("skill_id") or f"skill_{idx}"
        questions.append(
            {
                "question_id": f"q{idx}",
                "question_text": "Cau hoi ngan",
                "options": ["A", "B", "C", "D"],
                "correct_answer": "A",
                "skill_id": skill_id,
                "difficulty": "medium",
                "rationale": "",
            }
        )
    if not questions:
        questions = [
            {
                "question_id": "q1",
                "question_text": "Cau hoi ngan",
                "options": ["A", "B", "C", "D"],
                "correct_answer": "A",
                "skill_id": "",
                "difficulty": "medium",
                "rationale": "",
            }
        ]
    return {
        "questions": questions,
        "total_questions": len(questions),
        "skills_covered": [q.get("skill_id", "") for q in questions],
    }


def _normalize_group_dict(data: Dict[str, Any], idx: int) -> Dict[str, Any]:
    data.setdefault("group_id", f"group_{idx}")
    data.setdefault("group_name", f"G{idx}")
    data.setdefault("description", "")
    data.setdefault("mastery_level", "medium")
    if data.get("mastery_level") not in {"low", "medium", "high", "advanced"}:
        data["mastery_level"] = "medium"
    data.setdefault("skill_mastery", {})
    data.setdefault("common_misconceptions", [])
    data.setdefault("learning_pace", "moderate")
    if data.get("learning_pace") not in {"slow", "moderate", "fast"}:
        data["learning_pace"] = "moderate"
    data.setdefault("students", [])
    data.setdefault("recommended_activities", [])
    return data


def _normalize_pack_plan_dict(data: Dict[str, Any]) -> Dict[str, Any]:
    if "teaching_pack" in data and isinstance(data["teaching_pack"], dict):
        inner = data.pop("teaching_pack")
        for key in (
            "learning_objectives",
            "slide_outline",
            "quiz_blueprint",
            "estimated_time",
            "differentiation_strategy",
            "group_id",
        ):
            if key not in data and key in inner:
                data[key] = inner[key]
    data.setdefault("group_id", "")
    data.setdefault("learning_objectives", [])
    estimated_time = data.get("estimated_time")
    if isinstance(estimated_time, dict):
        data["estimated_time"] = sum(
            int(v) for v in estimated_time.values() if isinstance(v, (int, float, str)) and str(v).isdigit()
        )
    elif not isinstance(estimated_time, int):
        estimated_str = str(estimated_time)
        digits = "".join(ch for ch in estimated_str if ch.isdigit())
        data["estimated_time"] = int(digits) if digits else 0
    data.setdefault("differentiation_strategy", "")
    diff_strategy = data.get("differentiation_strategy")
    if not isinstance(diff_strategy, str):
        data["differentiation_strategy"] = json.dumps(diff_strategy, ensure_ascii=False)
    slide_outline = data.get("slide_outline")
    if isinstance(slide_outline, list):
        normalized_outline = []
        for idx, item in enumerate(slide_outline, start=1):
            if not isinstance(item, dict):
                continue
            fixed_item = dict(item)
            if "slide_number" in fixed_item:
                fixed_item["slide_number"] = str(fixed_item["slide_number"])
            else:
                fixed_item["slide_number"] = str(idx)
            key_points = fixed_item.get("key_points")
            if isinstance(key_points, list):
                fixed_item["key_points"] = "\n".join(str(x) for x in key_points)
            elif key_points is None:
                fixed_item["key_points"] = ""
            normalized_outline.append(fixed_item)
        data["slide_outline"] = normalized_outline
    data.setdefault("slide_outline", [])
    quiz_blueprint = data.get("quiz_blueprint")
    if isinstance(quiz_blueprint, dict):
        data["quiz_blueprint"] = [quiz_blueprint]
    elif quiz_blueprint is None:
        data["quiz_blueprint"] = []
    if isinstance(data.get("quiz_blueprint"), list):
        normalized_qb = []
        for qb in data["quiz_blueprint"]:
            if not isinstance(qb, dict):
                continue
            fixed_qb = dict(qb)
            for key in (
                "total_questions",
                "number_of_questions",
                "num_questions",
                "difficulty_levels",
                "question_types",
                "topics",
                "easy",
                "medium",
                "hard",
                "challenge",
            ):
                if key in fixed_qb:
                    val = fixed_qb[key]
                    if isinstance(val, list):
                        fixed_qb[key] = ", ".join(str(x) for x in val)
                    elif isinstance(val, dict):
                        fixed_qb[key] = json.dumps(val, ensure_ascii=False)
                    else:
                        fixed_qb[key] = str(val)
            normalized_qb.append(fixed_qb)
        data["quiz_blueprint"] = normalized_qb
    return data


def _fallback_pack_plan(
    group: Dict[str, Any],
    lesson_summary: Dict[str, Any],
    skill_set: Dict[str, Any],
) -> Dict[str, Any]:
    objectives = (lesson_summary.get("key_concepts") or [])[:2]
    slide_outline = [
        {"title": str(concept), "key_points": str(concept)}
        for concept in (lesson_summary.get("key_concepts") or [])[:3]
    ]
    if not slide_outline:
        slide_outline = [{"title": "Tong quan", "key_points": "Noi dung chinh"}]
    quiz_blueprint = []
    for skill in (skill_set.get("skills") or [])[:3]:
        quiz_blueprint.append({"skill_id": skill.get("skill_id", ""), "difficulty": "medium"})
    if not quiz_blueprint:
        quiz_blueprint = [{"skill_id": "", "difficulty": "medium"}]
    return {
        "group_id": group.get("group_id", ""),
        "learning_objectives": objectives,
        "slide_outline": slide_outline,
        "quiz_blueprint": quiz_blueprint,
        "estimated_time": 20,
        "differentiation_strategy": "",
    }


def _normalize_slides_dict(data: Dict[str, Any]) -> Dict[str, Any]:
    slides = data.get("slides")
    if isinstance(slides, list):
        normalized_slides = []
        for idx, slide in enumerate(slides, start=1):
            if not isinstance(slide, dict):
                continue
            fixed_slide = dict(slide)
            if "slide_id" not in fixed_slide:
                fixed_slide["slide_id"] = fixed_slide.get("id") or f"slide_{idx}"
            if "title" not in fixed_slide:
                fixed_slide["title"] = fixed_slide.get("slide_title") or fixed_slide.get("heading") or ""
            if "content" not in fixed_slide:
                fixed_slide["content"] = fixed_slide.get("body") or fixed_slide.get("text") or ""
            if "visual_notes" not in fixed_slide:
                fixed_slide["visual_notes"] = fixed_slide.get("visual_aids") or ""
            if "speaker_notes" not in fixed_slide:
                fixed_slide["speaker_notes"] = fixed_slide.get("notes") or ""
            normalized_slides.append(fixed_slide)
        data["slides"] = normalized_slides
    data.setdefault("slides", [])
    return data


def _fallback_slides(pack_plan: Dict[str, Any]) -> Dict[str, Any]:
    slides = []
    outline = pack_plan.get("slide_outline") or []
    for idx, item in enumerate(outline[:4], start=1):
        title = ""
        content = ""
        if isinstance(item, dict):
            title = str(item.get("title") or "")
            content = str(item.get("key_points") or "")
        slides.append(
            {
                "slide_id": f"slide_{idx}",
                "title": title or f"Slide {idx}",
                "content": content,
                "visual_notes": "",
                "speaker_notes": "",
            }
        )
    if not slides:
        slides = [
            {
                "slide_id": "slide_1",
                "title": "Tong quan",
                "content": "Noi dung chinh",
                "visual_notes": "",
                "speaker_notes": "",
            }
        ]
    return {"slides": slides, "generated_url": None}


def _normalize_quiz_dict(data: Dict[str, Any]) -> Dict[str, Any]:
    questions = data.get("questions")
    if isinstance(questions, list):
        normalized_qs = []
        for idx, q in enumerate(questions, start=1):
            if not isinstance(q, dict):
                continue
            fixed_q = dict(q)
            if "question_id" not in fixed_q:
                fixed_q["question_id"] = fixed_q.get("id") or f"q{idx}"
            if "question_text" not in fixed_q:
                fixed_q["question_text"] = fixed_q.get("question") or fixed_q.get("prompt") or ""
            if "correct_answer" not in fixed_q:
                fixed_q["correct_answer"] = fixed_q.get("answer") or fixed_q.get("correct") or ""
            diff = fixed_q.get("difficulty")
            diff_norm = str(diff).lower() if diff is not None else "medium"
            if diff_norm not in ("easy", "medium", "hard"):
                diff_norm = "medium"
            fixed_q["difficulty"] = diff_norm
            if "skill_id" not in fixed_q:
                fixed_q["skill_id"] = ""
            if "hint" not in fixed_q:
                fixed_q["hint"] = ""
            if "explanation" not in fixed_q:
                fixed_q["explanation"] = ""
            normalized_qs.append(fixed_q)
        data["questions"] = normalized_qs
    data.setdefault("questions", [])
    practice_exercises = data.get("practice_exercises")
    if isinstance(practice_exercises, list):
        normalized_ex = []
        for ex in practice_exercises:
            if not isinstance(ex, dict):
                continue
            fixed_ex = dict(ex)
            diff = fixed_ex.get("difficulty")
            diff_norm = str(diff).lower() if diff is not None else "medium"
            if diff_norm not in ("easy", "medium", "hard"):
                diff_norm = "medium"
            fixed_ex["difficulty"] = diff_norm
            normalized_ex.append(fixed_ex)
        data["practice_exercises"] = normalized_ex
    data.setdefault("practice_exercises", [])
    data.setdefault("answer_key", {})
    data.setdefault("total_questions", len(data.get("questions", [])))
    return data


def _fallback_quiz(skill_set: Dict[str, Any]) -> Dict[str, Any]:
    questions = []
    skills = skill_set.get("skills") or []
    for idx, skill in enumerate(skills[:3], start=1):
        skill_id = skill.get("skill_id", "")
        questions.append(
            {
                "question_id": f"q{idx}",
                "question_text": "Cau hoi ngan",
                "options": ["A", "B", "C", "D"],
                "correct_answer": "A",
                "skill_id": skill_id,
                "difficulty": "medium",
                "hint": "",
                "explanation": "",
            }
        )
    if not questions:
        questions = [
            {
                "question_id": "q1",
                "question_text": "Cau hoi ngan",
                "options": ["A", "B", "C", "D"],
                "correct_answer": "A",
                "skill_id": "",
                "difficulty": "medium",
                "hint": "",
                "explanation": "",
            }
        ]
    return {
        "questions": questions,
        "practice_exercises": [],
        "answer_key": {},
        "total_questions": len(questions),
        "estimated_time": 10,
    }


def _normalize_video_dict(data: Dict[str, Any]) -> Dict[str, Any]:
    data.setdefault("title", "Video")
    data.setdefault("duration_seconds", 0)
    data.setdefault("script", "")
    data.setdefault("visual_description", "")
    data.setdefault("key_concepts", [])
    data.setdefault("generated_url", None)
    data.setdefault("thumbnail_url", None)
    return data


def _fallback_video(lesson_summary: Dict[str, Any]) -> Dict[str, Any]:
    return {
        "title": lesson_summary.get("title") or "Video",
        "duration_seconds": 0,
        "script": "",
        "visual_description": "",
        "key_concepts": (lesson_summary.get("key_concepts") or [])[:3],
        "generated_url": None,
        "thumbnail_url": None,
    }


def _normalize_pipeline_dict(data: Dict[str, Any]) -> Dict[str, Any]:
    if "pipeline" in data and isinstance(data["pipeline"], dict):
        data = data["pipeline"]

    lesson_summary_raw = data.get("lesson_summary")
    if isinstance(lesson_summary_raw, dict):
        lesson_summary = _normalize_lesson_summary(lesson_summary_raw).model_dump()
    elif isinstance(lesson_summary_raw, str):
        lesson_summary = _lesson_summary_from_text(lesson_summary_raw)
    else:
        lesson_summary = _lesson_summary_from_text("")
    data["lesson_summary"] = lesson_summary

    skill_set_raw = data.get("skill_set")
    if isinstance(skill_set_raw, dict):
        skill_set = _normalize_skill_set_dict(skill_set_raw)
    else:
        skill_set = _fallback_skill_set(lesson_summary)
    data["skill_set"] = skill_set

    diagnostic_raw = data.get("diagnostic")
    if isinstance(diagnostic_raw, dict):
        diagnostic = _normalize_diagnostic_dict(diagnostic_raw)
    else:
        diagnostic = _fallback_diagnostic(skill_set)
    data["diagnostic"] = diagnostic

    groups = data.get("groups")
    normalized_groups = []
    if isinstance(groups, list) and groups:
        for idx, group in enumerate(groups, start=1):
            if isinstance(group, dict):
                normalized_groups.append(_normalize_group_dict(group, idx))
    else:
        packs = data.get("teaching_packs")
        if isinstance(packs, list):
            for idx, pack in enumerate(packs, start=1):
                if isinstance(pack, dict):
                    group = pack.get("group") if isinstance(pack.get("group"), dict) else pack
                    if isinstance(group, dict):
                        normalized_groups.append(_normalize_group_dict(group, idx))
    if not normalized_groups:
        num_groups = int(data.get("num_groups") or 1)
        for idx in range(1, num_groups + 1):
            normalized_groups.append(_normalize_group_dict({}, idx))
    data["groups"] = normalized_groups

    packs = data.get("teaching_packs")
    normalized_packs = []
    if isinstance(packs, list):
        for idx, group in enumerate(data["groups"], start=1):
            pack = packs[idx - 1] if idx - 1 < len(packs) else {}
            fixed_pack: Dict[str, Any] = {}
            if isinstance(pack, dict) and any(k in pack for k in ("pack_plan", "slides", "quiz", "video", "group")):
                fixed_pack.update(pack)
            fixed_pack["group"] = group
            pack_plan_raw = fixed_pack.get("pack_plan")
            if isinstance(pack_plan_raw, dict):
                pack_plan = _normalize_pack_plan_dict(pack_plan_raw)
            else:
                pack_plan = _fallback_pack_plan(group, lesson_summary, skill_set)
            pack_plan["group_id"] = group.get("group_id", "")
            fixed_pack["pack_plan"] = pack_plan
            slides_raw = fixed_pack.get("slides")
            if isinstance(slides_raw, dict):
                fixed_pack["slides"] = _normalize_slides_dict(slides_raw)
            else:
                fixed_pack["slides"] = _fallback_slides(pack_plan)
            quiz_raw = fixed_pack.get("quiz")
            if isinstance(quiz_raw, dict):
                fixed_pack["quiz"] = _normalize_quiz_dict(quiz_raw)
            else:
                fixed_pack["quiz"] = _fallback_quiz(skill_set)
            video_raw = fixed_pack.get("video")
            if isinstance(video_raw, dict):
                fixed_pack["video"] = _normalize_video_dict(video_raw)
            else:
                fixed_pack["video"] = _fallback_video(lesson_summary)
            normalized_packs.append(fixed_pack)
    else:
        for idx, group in enumerate(data["groups"], start=1):
            pack_plan = _fallback_pack_plan(group, lesson_summary, skill_set)
            normalized_packs.append(
                {
                    "group": group,
                    "pack_plan": pack_plan,
                    "slides": _fallback_slides(pack_plan),
                    "quiz": _fallback_quiz(skill_set),
                    "video": _fallback_video(lesson_summary),
                }
            )
    data["teaching_packs"] = normalized_packs

    return data


def _parse_pipeline_output(text: str) -> SingleAgentPipelineOutput:
    cleaned = _strip_code_fence(text)
    json_text = _extract_json_block(cleaned) if "{" in cleaned else cleaned
    try:
        return SingleAgentPipelineOutput.model_validate_json(json_text)
    except Exception:
        data = json.loads(json_text)
        if isinstance(data, dict):
            data = _normalize_pipeline_dict(data)
        return SingleAgentPipelineOutput.model_validate(data)


async def _run_agent_json(
    agent: Any,
    prompt: str,
    retries: int = 2,
    max_tokens: int = 320,
) -> SingleAgentPipelineOutput:
    last_err: Exception | None = None
    current_max_tokens = max_tokens
    parse_attempts = 0
    token_adjusts = 0
    json_adjusts = 0
    max_allowed: int | None = None
    while parse_attempts <= retries:
        try:
            result = await agent.run(prompt, model_settings={"max_tokens": current_max_tokens})
        except Exception as err:
            allowed = _extract_max_tokens_limit(err)
            if allowed is not None and token_adjusts < 10:
                max_allowed = allowed
                current_max_tokens = max(16, allowed - 64)
                last_err = err
                token_adjusts += 1
                continue
            msg = str(err)
            if (
                token_adjusts < 10
                and ("max_tokens" in msg or "maximum context length" in msg)
                and current_max_tokens > 32
            ):
                current_max_tokens = max(32, int(current_max_tokens * 0.6))
                last_err = err
                token_adjusts += 1
                continue
            raise
        raw = getattr(result, "data", None)
        if raw is None:
            raw = getattr(result, "output", None)
        if raw is None:
            raw = ""
        if not isinstance(raw, str):
            raw = str(raw)
        try:
            return _parse_pipeline_output(raw)
        except Exception as err:
            last_err = err
            err_text = str(err)
            if "Unclosed JSON object" in err_text and json_adjusts < 10:
                if max_allowed is None:
                    current_max_tokens += 64
                elif current_max_tokens < max_allowed - 8:
                    current_max_tokens = min(max_allowed - 8, current_max_tokens + 64)
                json_adjusts += 1
                continue
            parse_attempts += 1
            prompt = (
                prompt
                + "\n\nReturn ONLY a valid JSON object that strictly matches the required schema. "
                + "Do not include any extra text."
            )
    raise ValueError(f"Failed to parse model output as SingleAgentPipelineOutput: {last_err}")

class SingleAgentPipeline:
    """Runs the complete pipeline using a single agent call."""

    def __init__(
        self,
        vllm_base_url: str,
        vllm_model: str,
        vllm_api_key: str | None = None,
        vllm_lora: str | None = None,
    ):
        """Initialize the single pipeline agent."""
        provider = OpenAIProvider(base_url=vllm_base_url, api_key=vllm_api_key)
        extra_body = {"response_format": {"type": "json_object"}}
        if vllm_lora:
            extra_body["lora"] = vllm_lora
        self.model = OpenAIChatModel(
            vllm_model,
            provider=provider,
            settings={
                "extra_body": extra_body,
                "temperature": 0.2,
            },
        )

        self.pipeline_agent = AgentClient(
            system_prompt=SINGLE_AGENT_PIPELINE_PROMPT,
            tools=[],
            model=self.model
        ).create_agent()

    async def run_pipeline(
        self,
        lesson_summary: Optional[LessonSummary] = None,
        lesson_text: Optional[str] = None,
        num_groups: int = 3,
        num_students: int = 30,
        max_attempts: int = 3
    ) -> SingleAgentPipelineOutput:
        """
        Run the complete teaching-pack pipeline in a single agent call.

        Args:
            lesson_summary: Lesson summary JSON (preferred if available)
            lesson_text: Raw lesson text (used if lesson_summary is not provided)
            num_groups: Number of student groups (default: 3)
            num_students: Total number of students (default: 30)

        Returns:
            SingleAgentPipelineOutput with all pipeline artifacts
        """
        if lesson_summary is None and not lesson_text:
            raise ValueError("Either lesson_summary or lesson_text must be provided")

        print("=" * 80)
        print("STARTING SINGLE-AGENT PIPELINE")
        print("=" * 80)

        student_list = [f"S{i+1}" for i in range(num_students)]
        input_payload: Dict[str, Any] = {
            "num_groups": num_groups,
            "num_students": num_students,
            "student_list": student_list
        }

        if lesson_summary is not None:
            input_payload["lesson_summary"] = lesson_summary.model_dump()
        else:
            input_payload["lesson_text"] = lesson_text

        prompt = f"""# INPUT
{json.dumps(input_payload, ensure_ascii=False, indent=2)}
"""

        try:
            output = await _run_agent_json(
                self.pipeline_agent,
                prompt,
                retries=max(0, max_attempts - 1),
                max_tokens=320,
            )
        except ValueError as err:
            if "Unclosed JSON object" not in str(err):
                raise
            print("[WARN] Output truncated. Retrying in compact mode.")
            compact_num_groups = 1
            compact_num_students = min(10, num_students)
            compact_students = [f"S{i+1}" for i in range(compact_num_students)]
            compact_payload: Dict[str, Any] = {
                "num_groups": compact_num_groups,
                "num_students": compact_num_students,
                "student_list": compact_students,
            }
            if lesson_summary is not None:
                compact_payload["lesson_summary"] = lesson_summary.model_dump()
            else:
                compact_payload["lesson_text"] = _compact_raw_text(lesson_text or "", max_chars=800)

            compact_prompt = f"""# INPUT
{json.dumps(compact_payload, ensure_ascii=False, indent=2)}
"""
            compact_prompt += (
                "\n# COMPACT MODE\n"
                "Return minimal JSON only. Keep all fields very short."
            )
            output = await _run_agent_json(
                self.pipeline_agent,
                compact_prompt,
                retries=1,
                max_tokens=220,
            )

        print("\nPipeline complete!")
        print(f"   Completed: {len(output.teaching_packs)} teaching packs")

        return output

# =====================================================
# GEMINI EVALUATOR

EVALUATION_SYSTEM_PROMPT = '''You are an expert middle-school STEM teacher with experience evaluating instructional materials.

You are given:
1) A ground-truth lesson summary with key_concepts and skills
2) A generated teaching pack (slides + quiz)

Your task is to evaluate the teaching pack using three metrics:

A. Content Accuracy (Acc)
For each slide and each quiz question, judge factual correctness against the lesson summary.

Score each unit:
- 1.0 = fully correct and aligned with the lesson summary
- 0.5 = partially correct or ambiguous and could mislead
- 0.0 = incorrect or contradicts the lesson summary

B. Concept Coverage (EM)
This is semantic coverage, not exact wording.
Steps:
1) Extract all concepts from key_concepts and all skills from the skill set (if provided).
2) For each concept or skill, score coverage in the teaching pack.

Score each concept:
- 1.0 = clearly taught with correct meaning and context
- 0.5 = mentioned but incomplete, unclear, or shallow
- 0.0 = not covered

C. Educational Soundness (ES)
Score each criterion from 0 to 1:
1) Grade level appropriateness
2) Logical progression
3) Quiz alignment with taught content
4) Cognitive load management

General rules:
- Be strict and objective
- Provide short explanations for each score
- Return JSON that matches the EvaluationResult schema
'''

# =====================================================

class GeminiEvaluator:
    """Uses Gemini to evaluate teaching pack quality"""

    # Evaluation criteria for Educational Soundness
    EDUCATIONAL_CRITERIA = [
        "Grade level appropriateness (suitable for the target grade)",
        "Logical progression (easy to hard, building on prior knowledge)",
        "Quiz alignment (questions test the skills actually taught)",
        "Cognitive load management (not overwhelming, focused on objectives)"
    ]

    def __init__(self, gemini_api_key: str, gemini_model: str = DEFAULT_GEMINI_MODEL):
        """Initialize Gemini evaluator"""
        provider = GoogleProvider(api_key=gemini_api_key)
        self.model = GoogleModel(gemini_model, provider=provider)

        # Create evaluation agent
        self.eval_agent = PydanticAgent(
            model=self.model,
            output_type=EvaluationResult,
            system_prompt=EVALUATION_SYSTEM_PROMPT
        )

    @staticmethod
    def _clamp01(value: float) -> float:
        return max(0.0, min(1.0, float(value)))

    def _normalize_evaluation(self, evaluation: EvaluationResult) -> EvaluationResult:
        """Clamp all scores to [0,1] and recompute totals."""
        for score in evaluation.accuracy_scores:
            score.accuracy = self._clamp01(score.accuracy)
        for coverage in evaluation.concept_coverage:
            coverage.coverage = self._clamp01(coverage.coverage)
        for es in evaluation.educational_soundness:
            es.score = self._clamp01(es.score)

        if evaluation.accuracy_scores:
            evaluation.accuracy_total = sum(s.accuracy for s in evaluation.accuracy_scores) / len(evaluation.accuracy_scores)
        else:
            evaluation.accuracy_total = 0.0

        if evaluation.concept_coverage:
            evaluation.coverage_total = sum(c.coverage for c in evaluation.concept_coverage) / len(evaluation.concept_coverage)
        else:
            evaluation.coverage_total = 0.0

        if evaluation.educational_soundness:
            evaluation.educational_soundness_total = sum(s.score for s in evaluation.educational_soundness) / len(evaluation.educational_soundness)
        else:
            evaluation.educational_soundness_total = 0.0

        evaluation.overall_score = (
            0.4 * evaluation.accuracy_total
            + 0.3 * evaluation.coverage_total
            + 0.3 * evaluation.educational_soundness_total
        )
        evaluation.overall_score = self._clamp01(evaluation.overall_score)
        return evaluation

    async def evaluate(
        self,
        lesson_summary: LessonSummary,
        teaching_pack: Dict[str, Any],
        skill_set: Optional[SkillSet] = None,
        ground_truth: Optional[Dict[str, Any]] = None
    ) -> EvaluationResult:
        """
        Evaluate a teaching pack using Gemini as judge

        Args:
            lesson_summary: The original lesson summary (ground truth)
            teaching_pack: The generated teaching pack to evaluate
            skill_set: Optional skill set with all identified skills
            ground_truth: Optional additional ground truth data

        Returns:
            EvaluationResult with all metrics
        """
        print("\n" + "=" * 80)
        print("EVALUATING TEACHING PACK WITH GEMINI")
        print("=" * 80)

        # Prepare evaluation prompt
        slides = teaching_pack["slides"]
        quiz = teaching_pack["quiz"]
        group = teaching_pack["group"]

        # Build ground truth section
        ground_truth_section = f"""
# GROUND TRUTH LESSON SUMMARY

{lesson_summary.model_dump_json(indent=2)}
"""

        # Add skill set if provided (for complete concept coverage evaluation)
        if skill_set:
            ground_truth_section += f"""

# GROUND TRUTH SKILLS

{skill_set.model_dump_json(indent=2)}

**NOTE**: For Concept Coverage (EM) metric, evaluate coverage of BOTH:
- All key_concepts from lesson summary above
- All skills from the skill set above
"""

        evaluation_prompt = f"""
{ground_truth_section}

# GENERATED TEACHING PACK

## Group Profile
{group.model_dump_json(indent=2)}

## Slides (Total: {len(slides.slides)})
{slides.model_dump_json(indent=2)}

## Quiz (Total: {len(quiz.questions)} questions)
{quiz.model_dump_json(indent=2)}

---

# YOUR TASK

Evaluate this teaching pack according to the three metrics defined in your system prompt:
1. Content Accuracy (for each slide and quiz question)
2. Concept Coverage / Semantic Match (for EACH key concept AND skill from ground truth)
3. Educational Soundness (4 criteria)

IMPORTANT for Concept Coverage:
- Extract ALL concepts from lesson summary's "key_concepts" array
- Extract ALL skills from the skill set (if provided)
- Evaluate SEMANTIC coverage (not exact wording) for each one
- Score each: 1.0 (covered correctly), 0.5 (partial/unclear), 0.0 (not covered)

Be strict and objective. Provide brief explanations.
"""

        # Add ground truth if provided
        if ground_truth:
            evaluation_prompt = f"""
# ADDITIONAL GROUND TRUTH

{json.dumps(ground_truth, indent=2, ensure_ascii=False)}

{evaluation_prompt}
"""

        print("\nSending evaluation request to Gemini...")
        result = await self.eval_agent.run(evaluation_prompt)
        evaluation: EvaluationResult = self._normalize_evaluation(result.output)

        print("\n" + "=" * 80)
        print("EVALUATION COMPLETE")
        print("=" * 80)
        print(f"\n Content Accuracy:        {evaluation.accuracy_total:.2%} ({evaluation.num_slides} slides + {evaluation.num_quiz_questions} quiz questions)")
        print(f" Concept Coverage:        {evaluation.coverage_total:.2%} ({evaluation.num_concepts} concepts/skills evaluated)")
        print(f" Educational Soundness:   {evaluation.educational_soundness_total:.2%} (4 criteria)")
        print(f"\n OVERALL SCORE:           {evaluation.overall_score:.2%}")
        print(f"   Formula: 0.4Acc + 0.3EM + 0.3ES")
        print("=" * 80)

        return evaluation


# =====================================================
# MAIN EXPERIMENT RUNNER
# =====================================================

async def run_experiment(
    lesson_summary_path: str,
    ground_truth_path: Optional[str] = None,
    output_dir: str = "results/experiments",
    num_groups: int = 3,
    num_students: int = 30,
    vllm_base_url: str = "http://localhost:8000/v1",
    vllm_model: str = DEFAULT_VLLM_MODEL,
    vllm_api_key: str | None = None,
    vllm_lora: str | None = DEFAULT_VLLM_LORA,
    gemini_model: str = DEFAULT_GEMINI_MODEL,
):
    """
    Run the complete single-agent evaluation experiment

    Args:
        lesson_summary_path: Path to lesson summary JSON file
        ground_truth_path: Optional path to ground truth JSON file
        output_dir: Directory to save results
        num_groups: Number of student groups
        num_students: Total number of students
        vllm_base_url: vLLM OpenAI-compatible base URL
        vllm_model: Model name served by vLLM
        vllm_api_key: Optional API key for vLLM/OpenAI-compatible server
        vllm_lora: LoRA name registered in vLLM
        gemini_model: Gemini model used for evaluation
    """
    output_prefix = "QWEN3_GRPO_DPO_SINGLE_"
    # Load API key (prefer .env if present)
    _load_env_var_from_file("GEMINI_API_KEY")
    _load_env_var_from_file("VLLM_API_KEY")
    _load_env_var_from_file("OPENAI_API_KEY")
    gemini_api_key = os.getenv("GEMINI_API_KEY")
    if not gemini_api_key:
        raise ValueError("GEMINI_API_KEY environment variable not set")
    # Initialize pipeline and evaluator
    print("\n Initializing Single-Agent Pipeline...")
    vllm_api_key = vllm_api_key or os.getenv("VLLM_API_KEY") or os.getenv("OPENAI_API_KEY")
    pipeline = SingleAgentPipeline(vllm_base_url, vllm_model, vllm_api_key, vllm_lora)

    print(" Initializing Gemini Evaluator...")
    evaluator = GeminiEvaluator(gemini_api_key, gemini_model)

    # Load lesson summary (JSON) or lesson text (PDF)
    print(f"\n Loading lesson summary from: {lesson_summary_path}")
    lesson_summary_path_obj = Path(lesson_summary_path)
    lesson_summary: Optional[LessonSummary] = None
    lesson_text: Optional[str] = None
    raw_text: Optional[str] = None

    if lesson_summary_path_obj.suffix.lower() == ".pdf":
        raw_text = extract_text_from_pdf(str(lesson_summary_path_obj))
        print("    Loaded lesson text from PDF")
    else:
        with open(lesson_summary_path, 'r', encoding='utf-8') as f:
            lesson_data = json.load(f)

        # Check if it's a full teaching pack or just a lesson summary
        if "lesson_summary" in lesson_data:
            lesson_summary = LessonSummary(**lesson_data["lesson_summary"])
        else:
            lesson_summary = LessonSummary(**lesson_data)

        print(f"    Loaded: {lesson_summary.title}")
        print(f"    Subject: {lesson_summary.subject}")
        print(f"    Grade: {lesson_summary.grade}")
        print(f"    Key concepts: {len(lesson_summary.key_concepts)}")

    # Load ground truth if provided
    ground_truth = None
    gt_lesson_summary: Optional[LessonSummary] = None
    gt_skill_set: Optional[SkillSet] = None
    if ground_truth_path:
        print(f"\n Loading ground truth from: {ground_truth_path}")
        with open(ground_truth_path, 'r', encoding='utf-8') as f:
            ground_truth = json.load(f)
        print(f"    Ground truth loaded")

        if isinstance(ground_truth, dict):
            if "lesson_summary" in ground_truth:
                gt_lesson_summary = _normalize_lesson_summary(ground_truth["lesson_summary"])
            if "skill_set" in ground_truth:
                try:
                    gt_skill_set = SkillSet(**ground_truth["skill_set"])
                except Exception:
                    gt_skill_set = None

    # Run pipeline
    print("\n" + "=" * 80)
    print("PHASE 1: GENERATING TEACHING PACKS")
    print("=" * 80)
    if raw_text is not None:
        original_len = len(raw_text)
        max_chars = 1500
        attempt = 0
        while True:
            lesson_text = _compact_raw_text(raw_text, max_chars=max_chars)
            if attempt == 0 and len(lesson_text) < original_len:
                print(
                    f"[DEBUG] Trimmed lesson text from {original_len} to {len(lesson_text)} chars for context limits."
                )
            try:
                results = await pipeline.run_pipeline(
                    lesson_summary=None,
                    lesson_text=lesson_text,
                    num_groups=num_groups,
                    num_students=num_students
                )
                break
            except ModelHTTPError as err:
                if "maximum context length" in str(err) and max_chars > 800:
                    attempt += 1
                    max_chars = max(800, int(max_chars * 0.7))
                    print(f"[WARN] Context limit hit. Retrying with max_chars={max_chars}.")
                    continue
                raise
    else:
        results = await pipeline.run_pipeline(
            lesson_summary=lesson_summary,
            lesson_text=None,
            num_groups=num_groups,
            num_students=num_students
        )

    lesson_summary = results.lesson_summary
    skill_set = results.skill_set
    diagnostic = results.diagnostic
    groups = results.groups

    if lesson_summary_path_obj.suffix.lower() == ".pdf":
        grade_display = lesson_summary.grade or "Khng c ch nh trong ti liu"
        print(f"\n    Loaded: {lesson_summary.title}")
        print(f"    Subject: {lesson_summary.subject}")
        print(f"    Grade: {grade_display}")
        print(f"    Key concepts: {len(lesson_summary.key_concepts)}")

    # Export teaching pack file (same format as teaching_packs_*.json in output_dir)
    serialized_packs = [
        {
            "group": tp.group.model_dump(),
            "pack_plan": tp.pack_plan.model_dump(),
            "slides": tp.slides.model_dump(),
            "video": tp.video.model_dump(),
            "quiz": tp.quiz.model_dump(),
        }
        for tp in results.teaching_packs
    ]
    output_file_name = export_final_results(
        lesson_summary,
        skill_set,
        diagnostic,
        groups,
        serialized_packs,
        num_students,
        output_dir=output_dir,
    )
    teaching_pack_output_path = Path(output_dir) / output_file_name
    prefixed_pack_name = f"{output_prefix}{output_file_name}"
    prefixed_pack_path = teaching_pack_output_path.parent / prefixed_pack_name
    teaching_pack_output_path.replace(prefixed_pack_path)
    teaching_pack_output_path = prefixed_pack_path
    print(f"\n Teaching pack exported: {teaching_pack_output_path}")

    # Reload from exported file to ensure evaluation uses the generated JSON output
    teaching_packs_for_eval: List[Dict[str, Any]] = []
    with open(teaching_pack_output_path, 'r', encoding='utf-8') as f:
        exported_data = json.load(f)
    for pack in exported_data.get("teaching_packs", []):
        teaching_packs_for_eval.append({
            "group": GroupProfile(**pack["group"]),
            "pack_plan": PackPlan(**pack["pack_plan"]),
            "slides": Slides(**pack["slides"]),
            "video": Video(**pack["video"]),
            "quiz": Quiz(**pack["quiz"]),
        })

    # Evaluate each teaching pack
    print("\n" + "=" * 80)
    print("PHASE 2: EVALUATING TEACHING PACKS")
    print("=" * 80)

    evaluations = []
    eval_lesson_summary = gt_lesson_summary or lesson_summary
    eval_skill_set = gt_skill_set or skill_set

    for i, teaching_pack in enumerate(teaching_packs_for_eval):
        print(f"\n Evaluating teaching pack {i+1}/{len(teaching_packs_for_eval)}...")
        print(f"   Group: {teaching_pack['group'].group_name}")

        evaluation = await evaluator.evaluate(
            lesson_summary=eval_lesson_summary,
            teaching_pack=teaching_pack,
            skill_set=eval_skill_set,
            ground_truth=ground_truth
        )

        evaluations.append({
            "group_id": teaching_pack["group"].group_id,
            "group_name": teaching_pack["group"].group_name,
            "evaluation": evaluation
        })

    # Save results
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)

    # Save complete results
    results_file = output_path / f"{output_prefix}experiment_results_{timestamp}.json"
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump({
            "timestamp": timestamp,
            "lesson_summary": lesson_summary.model_dump(),
            "skill_set": skill_set.model_dump() if skill_set else None,
            "num_groups": num_groups,
            "num_students": num_students,
            "teaching_packs": serialized_packs,
            "teaching_pack_output_file": str(teaching_pack_output_path),
            "evaluations": [
                {
                    "group_id": e["group_id"],
                    "group_name": e["group_name"],
                    "evaluation": e["evaluation"].model_dump()
                }
                for e in evaluations
            ]
        }, f, indent=2, ensure_ascii=False)

    print(f"\n Results saved to: {results_file}")

    # Print summary
    print("\n" + "=" * 80)
    print("EXPERIMENT SUMMARY")
    print("=" * 80)

    for eval_result in evaluations:
        eval_data = eval_result["evaluation"]
        print(f"\n {eval_result['group_name']}")
        print(f"   Content Accuracy:       {eval_data.accuracy_total:.2%}")
        print(f"   Concept Coverage:       {eval_data.coverage_total:.2%}")
        print(f"   Educational Soundness:  {eval_data.educational_soundness_total:.2%}")
        print(f"   Overall Score:          {eval_data.overall_score:.2%}")

    # Calculate average scores
    avg_accuracy = sum(e["evaluation"].accuracy_total for e in evaluations) / len(evaluations)
    avg_coverage = sum(e["evaluation"].coverage_total for e in evaluations) / len(evaluations)
    avg_soundness = sum(e["evaluation"].educational_soundness_total for e in evaluations) / len(evaluations)
    avg_overall = sum(e["evaluation"].overall_score for e in evaluations) / len(evaluations)

    print("\n" + "=" * 80)
    print("AVERAGE SCORES ACROSS ALL GROUPS")
    print("=" * 80)
    print(f"\n Avg Content Accuracy:       {avg_accuracy:.2%}")
    print(f" Avg Concept Coverage:       {avg_coverage:.2%}")
    print(f" Avg Educational Soundness:  {avg_soundness:.2%}")
    print(f"\n AVG OVERALL SCORE:          {avg_overall:.2%}")
    print("=" * 80)


# =====================================================
# CLI ENTRY POINT
# =====================================================

def main():
    parser = argparse.ArgumentParser(
        description="Run single-agent evaluation with Qwen3-4B SFT-GRPO-DPO (vLLM LoRA) and Gemini 2.0 Flash"
    )
    parser.add_argument(
        "--lesson_summary",
        type=str,
        required=True,
        help="Path to lesson summary JSON file or PDF lesson file"
    )
    parser.add_argument(
        "--config",
        type=str,
        default="config/default.yaml",
        help="Path to YAML config file (default: config/default.yaml)"
    )
    parser.add_argument(
        "--ground_truth",
        type=str,
        default=None,
        help="Path to ground truth JSON file (optional)"
    )
    parser.add_argument(
        "--output_dir",
        type=str,
        default=None,
        help="Directory to save results (default: from config)"
    )
    parser.add_argument(
        "--num_groups",
        type=int,
        default=None,
        help="Number of student groups (default: from config)"
    )
    parser.add_argument(
        "--num_students",
        type=int,
        default=None,
        help="Total number of students (default: from config)"
    )
    parser.add_argument(
        "--vllm_base_url",
        type=str,
        default=None,
        help="vLLM OpenAI-compatible base URL (default: from config)"
    )
    parser.add_argument(
        "--vllm_model",
        type=str,
        default=None,
        help="Model name served by vLLM (default: from config)"
    )
    parser.add_argument(
        "--vllm_lora",
        type=str,
        default=None,
        help="LoRA name registered in vLLM (default: from config)"
    )
    parser.add_argument(
        "--vllm_api_key",
        type=str,
        default=None,
        help="Optional API key for vLLM/OpenAI-compatible server"
    )
    parser.add_argument(
        "--gemini_model",
        type=str,
        default=None,
        help="Gemini model to use for evaluation (default: from config)"
    )
    parser.add_argument(
        "--seed",
        type=int,
        default=None,
        help="Random seed (default: from config)"
    )

    args = parser.parse_args()

    config = load_config(args.config)
    num_groups = resolve_value(args.num_groups, config, ["evaluation", "num_groups"], 3)
    num_students = resolve_value(args.num_students, config, ["evaluation", "num_students"], 30)
    output_dir = resolve_value(args.output_dir, config, ["paths", "output_dir"], "results/experiments")
    seed = resolve_value(args.seed, config, ["seed"], None)

    vllm_base_url = resolve_value(args.vllm_base_url, config, ["models", "vllm", "base_url"], "http://localhost:8000/v1")
    vllm_model = resolve_value(args.vllm_model, config, ["models", "vllm", "model"], DEFAULT_VLLM_MODEL)
    vllm_lora = resolve_value(args.vllm_lora, config, ["models", "vllm", "lora"], DEFAULT_VLLM_LORA)
    vllm_api_key = resolve_value(args.vllm_api_key, config, ["models", "vllm", "api_key"], None)
    gemini_model = resolve_value(args.gemini_model, config, ["models", "single_agent", "evaluator"], DEFAULT_GEMINI_MODEL)

    set_seed(seed)

    # Run experiment
    asyncio.run(run_experiment(
        lesson_summary_path=args.lesson_summary,
        ground_truth_path=args.ground_truth,
        output_dir=output_dir,
        num_groups=num_groups,
        num_students=num_students,
        vllm_base_url=vllm_base_url,
        vllm_model=vllm_model,
        vllm_api_key=vllm_api_key,
        vllm_lora=vllm_lora,
        gemini_model=gemini_model,
    ))


if __name__ == "__main__":
    main()
