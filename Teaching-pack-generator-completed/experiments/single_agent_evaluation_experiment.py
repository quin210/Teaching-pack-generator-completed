"""
Single-Agent Evaluation Experiment
==================================

This script evaluates a single-agent pipeline for teaching pack generation using three metrics:
1. Content Accuracy (Acc) - Factual correctness of slides and quiz
2. Exact Match / Semantic Coverage (EM) - Coverage of key concepts from lesson summary
3. Educational Soundness (ES) - Pedagogical appropriateness

Usage:
    python experiments/single_agent_evaluation_experiment.py --lesson_summary <path> --ground_truth <path>
"""

import os
import sys
import json
import asyncio
import argparse
from datetime import datetime
from typing import Dict, List, Any, Optional
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# Import Single-Agent pipeline components
from src.llm.base import AgentClient
from src.utils.config_loader import load_config, resolve_value, get_config_value
from src.utils.reproducibility import set_seed
from src.models.teaching_pack_models import (
    LessonSummary,
    SkillSet,
    Diagnostic,
    GroupProfile,
    PackPlan,
    Slides,
    Video,
    Quiz,
)
from src.utils.workflow_helpers import export_final_results
from src.utils.basetools.pdf_parser import extract_text_from_pdf

# Import Google Gemini for evaluation
from pydantic_ai import Agent as PydanticAgent
from pydantic_ai.exceptions import UnexpectedModelBehavior
from pydantic_ai.models.google import GoogleModel
from pydantic_ai.providers.google import GoogleProvider
from pydantic import BaseModel


# =====================================================
# EVALUATION RESULT SCHEMAS
# =====================================================

class UnitScore(BaseModel):
    """Score for a single unit (slide or quiz question)"""
    unit_id: str
    unit_type: str  # "slide" or "quiz"
    accuracy: float  # 0, 0.5, or 1
    explanation: str


class ConceptCoverage(BaseModel):
    """Coverage score for a single concept"""
    concept: str
    coverage: float  # 0, 0.5, or 1
    explanation: str


class EducationalSoundnessScore(BaseModel):
    """Educational soundness evaluation"""
    criterion: str
    score: float  # 0 to 1
    explanation: str


class EvaluationResult(BaseModel):
    """Complete evaluation result with all metrics"""
    # Content Accuracy
    accuracy_scores: List[UnitScore]
    accuracy_total: float

    # Exact Match / Semantic Coverage
    concept_coverage: List[ConceptCoverage]
    coverage_total: float

    # Educational Soundness
    educational_soundness: List[EducationalSoundnessScore]
    educational_soundness_total: float

    # Overall Score
    overall_score: float

    # Metadata
    num_slides: int
    num_quiz_questions: int
    num_concepts: int
    evaluation_timestamp: str


# =====================================================
# ENV LOADER
# =====================================================

def _load_env_var_from_file(var_name: str, env_path: str = ".env") -> None:
    """Load a single env var from a .env file if not already set."""
    if os.getenv(var_name):
        return
    if not os.path.exists(env_path):
        return
    try:
        with open(env_path, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line or line.startswith("#"):
                    continue
                if line.startswith(var_name + "="):
                    value = line.split("=", 1)[1].strip().strip('"').strip("'")
                    if value:
                        os.environ[var_name] = value
                    return
    except Exception:
        # Fail silently; caller will validate env vars later
        return


# =====================================================
# SINGLE-AGENT PIPELINE SCHEMAS
# =====================================================

class TeachingPackBundle(BaseModel):
    """Container for a full teaching pack per group."""
    group: GroupProfile
    pack_plan: PackPlan
    slides: Slides
    video: Video
    quiz: Quiz


class SingleAgentPipelineOutput(BaseModel):
    """Complete pipeline output generated by a single agent."""
    lesson_summary: LessonSummary
    skill_set: SkillSet
    diagnostic: Diagnostic
    groups: List[GroupProfile]
    teaching_packs: List[TeachingPackBundle]


SINGLE_AGENT_PIPELINE_PROMPT = """You are an expert instructional designer and system orchestrator.

Task: In ONE response, complete the entire teaching-pack pipeline.

Return JSON that matches the SingleAgentPipelineOutput schema.

Input will provide:
- lesson_summary (optional, JSON)
- lesson_text (optional, raw text)
- num_groups, num_students, student_list

Rules:
- If lesson_summary is provided, copy it verbatim into output (do not rewrite).
- If lesson_text is provided, derive lesson_summary strictly from it (no outside knowledge).
- Use the same language as the input and keep it age appropriate.
- Use only allowed enum values:
  - mastery_level: "low" | "medium" | "high" | "advanced"
  - learning_pace: "slow" | "moderate" | "fast"
  - difficulty: "easy" | "medium" | "hard"
- groups length == num_groups; group_id values "group_1"...; students partitioned from student_list, each exactly once.
- teaching_packs length == num_groups and order matches groups; teaching_packs[i].group must equal groups[i].
- pack_plan.group_id must match group.group_id.
- pack_plan.slide_outline items must include "title" and "key_points" fields.
- pack_plan.quiz_blueprint items must include "skill_id" and "difficulty" fields.
- Diagnostic: 5-10 questions; cover all skills; total_questions equals len(questions); skills_covered list matches skill_ids used.
- Slides: 6-8 slides, slide_id "slide_1"...; generated_url must be null.
- Video: generated_url and thumbnail_url must be null.
- Quiz: align with pack_plan quiz_blueprint; total_questions equals len(questions); estimated_time in minutes.

Return JSON only, no extra text.
"""


# =====================================================
# SINGLE-AGENT PIPELINE RUNNER
# =====================================================

def _normalize_lesson_summary(raw: Dict[str, Any]) -> LessonSummary:
    """Coerce ground-truth lesson summary into the expected schema."""
    data = dict(raw)
    data["grade"] = str(data.get("grade", ""))
    if "key_concepts" in data:
        data["key_concepts"] = [str(x) for x in (data.get("key_concepts") or [])]
    if "examples" in data:
        data["examples"] = [str(x) for x in (data.get("examples") or [])]
    if "definitions" in data and isinstance(data["definitions"], list):
        defs: Dict[str, str] = {}
        for item in data["definitions"]:
            if isinstance(item, dict):
                term = item.get("term") or item.get("key") or item.get("name")
                definition = item.get("definition") or item.get("value") or item.get("desc")
                if term is not None and definition is not None:
                    defs[str(term)] = str(definition)
        data["definitions"] = defs
    data.setdefault("lesson_content", "")
    return LessonSummary(**data)

class SingleAgentPipeline:
    """Runs the complete pipeline using a single agent call."""

    def __init__(self, gemini_api_key: str, model_name: str):
        """Initialize the single pipeline agent."""
        provider = GoogleProvider(api_key=gemini_api_key)
        self.model = GoogleModel(model_name, provider=provider)

        self.pipeline_agent = AgentClient(
            system_prompt=SINGLE_AGENT_PIPELINE_PROMPT,
            tools=[],
            model=self.model
        ).create_agent(result_type=SingleAgentPipelineOutput)

    async def run_pipeline(
        self,
        lesson_summary: Optional[LessonSummary] = None,
        lesson_text: Optional[str] = None,
        num_groups: int = 3,
        num_students: int = 30,
        max_attempts: int = 3
    ) -> SingleAgentPipelineOutput:
        """
        Run the complete teaching-pack pipeline in a single agent call.

        Args:
            lesson_summary: Lesson summary JSON (preferred if available)
            lesson_text: Raw lesson text (used if lesson_summary is not provided)
            num_groups: Number of student groups (default: 3)
            num_students: Total number of students (default: 30)

        Returns:
            SingleAgentPipelineOutput with all pipeline artifacts
        """
        if lesson_summary is None and not lesson_text:
            raise ValueError("Either lesson_summary or lesson_text must be provided")

        print("=" * 80)
        print("STARTING SINGLE-AGENT PIPELINE")
        print("=" * 80)

        student_list = [f"Student_{i+1}" for i in range(num_students)]
        input_payload: Dict[str, Any] = {
            "num_groups": num_groups,
            "num_students": num_students,
            "student_list": student_list
        }

        if lesson_summary is not None:
            input_payload["lesson_summary"] = lesson_summary.model_dump()
        else:
            input_payload["lesson_text"] = lesson_text

        prompt = f"""# INPUT
{json.dumps(input_payload, ensure_ascii=False, indent=2)}
"""

        last_error: Optional[Exception] = None
        output: Optional[SingleAgentPipelineOutput] = None
        current_prompt = prompt
        for attempt in range(1, max_attempts + 1):
            try:
                if attempt > 1:
                    print(f"\nRetrying single-agent pipeline (attempt {attempt}/{max_attempts})...")
                    current_prompt = (
                        prompt
                        + "\n\n# VALIDATION ERROR\n"
                        + "The previous output failed schema validation. "
                        + "Return ONLY valid JSON that matches SingleAgentPipelineOutput exactly."
                    )
                result = await self.pipeline_agent.run(current_prompt)
                output: SingleAgentPipelineOutput = result.output
                break
            except UnexpectedModelBehavior as exc:
                last_error = exc
        else:
            raise last_error or UnexpectedModelBehavior("Single-agent pipeline failed validation.")

        if output is None:
            raise last_error or UnexpectedModelBehavior("Single-agent pipeline failed validation.")

        print("\nPipeline complete!")
        print(f"   Completed: {len(output.teaching_packs)} teaching packs")

        return output

# =====================================================
# GEMINI EVALUATOR

EVALUATION_SYSTEM_PROMPT = '''You are an expert middle-school STEM teacher with experience evaluating instructional materials.

You are given:
1) A ground-truth lesson summary with key_concepts and skills
2) A generated teaching pack (slides + quiz)

Your task is to evaluate the teaching pack using three metrics:

A. Content Accuracy (Acc)
For each slide and each quiz question, judge factual correctness against the lesson summary.

Score each unit:
- 1.0 = fully correct and aligned with the lesson summary
- 0.5 = partially correct or ambiguous and could mislead
- 0.0 = incorrect or contradicts the lesson summary

B. Concept Coverage (EM)
This is semantic coverage, not exact wording.
Steps:
1) Extract all concepts from key_concepts and all skills from the skill set (if provided).
2) For each concept or skill, score coverage in the teaching pack.

Score each concept:
- 1.0 = clearly taught with correct meaning and context
- 0.5 = mentioned but incomplete, unclear, or shallow
- 0.0 = not covered

C. Educational Soundness (ES)
Score each criterion from 0 to 1:
1) Grade level appropriateness
2) Logical progression
3) Quiz alignment with taught content
4) Cognitive load management

General rules:
- Be strict and objective
- Provide short explanations for each score
- Return JSON that matches the EvaluationResult schema
'''

# =====================================================

class GeminiEvaluator:
    """Uses Gemini to evaluate teaching pack quality"""

    # Evaluation criteria for Educational Soundness
    EDUCATIONAL_CRITERIA = [
        "Grade level appropriateness (suitable for the target grade)",
        "Logical progression (easy to hard, building on prior knowledge)",
        "Quiz alignment (questions test the skills actually taught)",
        "Cognitive load management (not overwhelming, focused on objectives)"
    ]

    def __init__(self, gemini_api_key: str, model_name: str):
        """Initialize Gemini evaluator"""
        provider = GoogleProvider(api_key=gemini_api_key)
        self.model = GoogleModel(model_name, provider=provider)

        # Create evaluation agent
        self.eval_agent = PydanticAgent(
            model=self.model,
            output_type=EvaluationResult,
            system_prompt=EVALUATION_SYSTEM_PROMPT
        )

    @staticmethod
    def _clamp01(value: float) -> float:
        return max(0.0, min(1.0, float(value)))

    def _normalize_evaluation(self, evaluation: EvaluationResult) -> EvaluationResult:
        """Clamp all scores to [0,1] and recompute totals."""
        for score in evaluation.accuracy_scores:
            score.accuracy = self._clamp01(score.accuracy)
        for coverage in evaluation.concept_coverage:
            coverage.coverage = self._clamp01(coverage.coverage)
        for es in evaluation.educational_soundness:
            es.score = self._clamp01(es.score)

        if evaluation.accuracy_scores:
            evaluation.accuracy_total = sum(s.accuracy for s in evaluation.accuracy_scores) / len(evaluation.accuracy_scores)
        else:
            evaluation.accuracy_total = 0.0

        if evaluation.concept_coverage:
            evaluation.coverage_total = sum(c.coverage for c in evaluation.concept_coverage) / len(evaluation.concept_coverage)
        else:
            evaluation.coverage_total = 0.0

        if evaluation.educational_soundness:
            evaluation.educational_soundness_total = sum(s.score for s in evaluation.educational_soundness) / len(evaluation.educational_soundness)
        else:
            evaluation.educational_soundness_total = 0.0

        evaluation.overall_score = (
            0.4 * evaluation.accuracy_total
            + 0.3 * evaluation.coverage_total
            + 0.3 * evaluation.educational_soundness_total
        )
        evaluation.overall_score = self._clamp01(evaluation.overall_score)
        return evaluation

    async def evaluate(
        self,
        lesson_summary: LessonSummary,
        teaching_pack: Dict[str, Any],
        skill_set: Optional[SkillSet] = None,
        ground_truth: Optional[Dict[str, Any]] = None
    ) -> EvaluationResult:
        """
        Evaluate a teaching pack using Gemini as judge

        Args:
            lesson_summary: The original lesson summary (ground truth)
            teaching_pack: The generated teaching pack to evaluate
            skill_set: Optional skill set with all identified skills
            ground_truth: Optional additional ground truth data

        Returns:
            EvaluationResult with all metrics
        """
        print("\n" + "=" * 80)
        print("EVALUATING TEACHING PACK WITH GEMINI")
        print("=" * 80)

        # Prepare evaluation prompt
        slides = teaching_pack["slides"]
        quiz = teaching_pack["quiz"]
        group = teaching_pack["group"]

        # Build ground truth section
        ground_truth_section = f"""
# GROUND TRUTH LESSON SUMMARY

{lesson_summary.model_dump_json(indent=2)}
"""

        # Add skill set if provided (for complete concept coverage evaluation)
        if skill_set:
            ground_truth_section += f"""

# GROUND TRUTH SKILLS

{skill_set.model_dump_json(indent=2)}

**NOTE**: For Concept Coverage (EM) metric, evaluate coverage of BOTH:
- All key_concepts from lesson summary above
- All skills from the skill set above
"""

        evaluation_prompt = f"""
{ground_truth_section}

# GENERATED TEACHING PACK

## Group Profile
{group.model_dump_json(indent=2)}

## Slides (Total: {len(slides.slides)})
{slides.model_dump_json(indent=2)}

## Quiz (Total: {len(quiz.questions)} questions)
{quiz.model_dump_json(indent=2)}

---

# YOUR TASK

Evaluate this teaching pack according to the three metrics defined in your system prompt:
1. Content Accuracy (for each slide and quiz question)
2. Concept Coverage / Semantic Match (for EACH key concept AND skill from ground truth)
3. Educational Soundness (4 criteria)

IMPORTANT for Concept Coverage:
- Extract ALL concepts from lesson summary's "key_concepts" array
- Extract ALL skills from the skill set (if provided)
- Evaluate SEMANTIC coverage (not exact wording) for each one
- Score each: 1.0 (covered correctly), 0.5 (partial/unclear), 0.0 (not covered)

Be strict and objective. Provide brief explanations.
"""

        # Add ground truth if provided
        if ground_truth:
            evaluation_prompt = f"""
# ADDITIONAL GROUND TRUTH

{json.dumps(ground_truth, indent=2, ensure_ascii=False)}

{evaluation_prompt}
"""

        print("\nSending evaluation request to Gemini...")
        result = await self.eval_agent.run(evaluation_prompt)
        evaluation: EvaluationResult = self._normalize_evaluation(result.output)

        print("\n" + "=" * 80)
        print("EVALUATION COMPLETE")
        print("=" * 80)
        print(f"\n Content Accuracy:        {evaluation.accuracy_total:.2%} ({evaluation.num_slides} slides + {evaluation.num_quiz_questions} quiz questions)")
        print(f" Concept Coverage:        {evaluation.coverage_total:.2%} ({evaluation.num_concepts} concepts/skills evaluated)")
        print(f" Educational Soundness:   {evaluation.educational_soundness_total:.2%} (4 criteria)")
        print(f"\n OVERALL SCORE:           {evaluation.overall_score:.2%}")
        print(f"   Formula: 0.4Acc + 0.3EM + 0.3ES")
        print("=" * 80)

        return evaluation


# =====================================================
# MAIN EXPERIMENT RUNNER
# =====================================================

async def run_experiment(
    lesson_summary_path: str,
    ground_truth_path: Optional[str] = None,
    output_dir: str = "results/experiments",
    num_groups: int = 3,
    num_students: int = 30,
    pipeline_model: str = "gemini-2.0-flash",
    evaluator_model: str = "gemini-2.5-flash",
):
    """
    Run the complete single-agent evaluation experiment

    Args:
        lesson_summary_path: Path to lesson summary JSON file
        ground_truth_path: Optional path to ground truth JSON file
        output_dir: Directory to save results
        num_groups: Number of student groups
        num_students: Total number of students
    """
    output_prefix = "GEMINI_NO_AGENT_"
    # Load API key (prefer .env if present)
    _load_env_var_from_file("GEMINI_API_KEY")
    gemini_api_key = os.getenv("GEMINI_API_KEY")
    if not gemini_api_key:
        raise ValueError("GEMINI_API_KEY environment variable not set")
    # Initialize pipeline and evaluator
    print("\n Initializing Single-Agent Pipeline...")
    pipeline = SingleAgentPipeline(gemini_api_key, pipeline_model)

    print(" Initializing Gemini Evaluator...")
    evaluator = GeminiEvaluator(gemini_api_key, evaluator_model)

    # Load lesson summary (JSON) or lesson text (PDF)
    print(f"\n Loading lesson summary from: {lesson_summary_path}")
    lesson_summary_path_obj = Path(lesson_summary_path)
    lesson_summary: Optional[LessonSummary] = None
    lesson_text: Optional[str] = None

    if lesson_summary_path_obj.suffix.lower() == ".pdf":
        lesson_text = extract_text_from_pdf(str(lesson_summary_path_obj))
        print("    Loaded lesson text from PDF")
    else:
        with open(lesson_summary_path, 'r', encoding='utf-8') as f:
            lesson_data = json.load(f)

        # Check if it's a full teaching pack or just a lesson summary
        if "lesson_summary" in lesson_data:
            lesson_summary = LessonSummary(**lesson_data["lesson_summary"])
        else:
            lesson_summary = LessonSummary(**lesson_data)

        print(f"    Loaded: {lesson_summary.title}")
        print(f"    Subject: {lesson_summary.subject}")
        print(f"    Grade: {lesson_summary.grade}")
        print(f"    Key concepts: {len(lesson_summary.key_concepts)}")

    # Load ground truth if provided
    ground_truth = None
    gt_lesson_summary: Optional[LessonSummary] = None
    gt_skill_set: Optional[SkillSet] = None
    if ground_truth_path:
        print(f"\n Loading ground truth from: {ground_truth_path}")
        with open(ground_truth_path, 'r', encoding='utf-8') as f:
            ground_truth = json.load(f)
        print(f"    Ground truth loaded")

        if isinstance(ground_truth, dict):
            if "lesson_summary" in ground_truth:
                gt_lesson_summary = _normalize_lesson_summary(ground_truth["lesson_summary"])
            if "skill_set" in ground_truth:
                try:
                    gt_skill_set = SkillSet(**ground_truth["skill_set"])
                except Exception:
                    gt_skill_set = None

    # Run pipeline
    print("\n" + "=" * 80)
    print("PHASE 1: GENERATING TEACHING PACKS")
    print("=" * 80)
    results = await pipeline.run_pipeline(
        lesson_summary=lesson_summary,
        lesson_text=lesson_text,
        num_groups=num_groups,
        num_students=num_students
    )

    lesson_summary = results.lesson_summary
    skill_set = results.skill_set
    diagnostic = results.diagnostic
    groups = results.groups

    if lesson_summary_path_obj.suffix.lower() == ".pdf":
        grade_display = lesson_summary.grade or "Khng c ch nh trong ti liu"
        print(f"\n    Loaded: {lesson_summary.title}")
        print(f"    Subject: {lesson_summary.subject}")
        print(f"    Grade: {grade_display}")
        print(f"    Key concepts: {len(lesson_summary.key_concepts)}")

    # Export teaching pack file (same format as teaching_packs_*.json in output_dir)
    serialized_packs = [
        {
            "group": tp.group.model_dump(),
            "pack_plan": tp.pack_plan.model_dump(),
            "slides": tp.slides.model_dump(),
            "video": tp.video.model_dump(),
            "quiz": tp.quiz.model_dump(),
        }
        for tp in results.teaching_packs
    ]
    output_file_name = export_final_results(
        lesson_summary,
        skill_set,
        diagnostic,
        groups,
        serialized_packs,
        num_students,
        output_dir=output_dir,
    )
    teaching_pack_output_path = Path(output_dir) / output_file_name
    prefixed_pack_name = f"{output_prefix}{output_file_name}"
    prefixed_pack_path = teaching_pack_output_path.parent / prefixed_pack_name
    teaching_pack_output_path.replace(prefixed_pack_path)
    teaching_pack_output_path = prefixed_pack_path
    print(f"\n Teaching pack exported: {teaching_pack_output_path}")

    # Reload from exported file to ensure evaluation uses the generated JSON output
    teaching_packs_for_eval: List[Dict[str, Any]] = []
    with open(teaching_pack_output_path, 'r', encoding='utf-8') as f:
        exported_data = json.load(f)
    for pack in exported_data.get("teaching_packs", []):
        teaching_packs_for_eval.append({
            "group": GroupProfile(**pack["group"]),
            "pack_plan": PackPlan(**pack["pack_plan"]),
            "slides": Slides(**pack["slides"]),
            "video": Video(**pack["video"]),
            "quiz": Quiz(**pack["quiz"]),
        })

    # Evaluate each teaching pack
    print("\n" + "=" * 80)
    print("PHASE 2: EVALUATING TEACHING PACKS")
    print("=" * 80)

    evaluations = []
    eval_lesson_summary = gt_lesson_summary or lesson_summary
    eval_skill_set = gt_skill_set or skill_set

    for i, teaching_pack in enumerate(teaching_packs_for_eval):
        print(f"\n Evaluating teaching pack {i+1}/{len(teaching_packs_for_eval)}...")
        print(f"   Group: {teaching_pack['group'].group_name}")

        evaluation = await evaluator.evaluate(
            lesson_summary=eval_lesson_summary,
            teaching_pack=teaching_pack,
            skill_set=eval_skill_set,
            ground_truth=ground_truth
        )

        evaluations.append({
            "group_id": teaching_pack["group"].group_id,
            "group_name": teaching_pack["group"].group_name,
            "evaluation": evaluation
        })

    # Save results
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)

    # Save complete results
    results_file = output_path / f"{output_prefix}experiment_results_{timestamp}.json"
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump({
            "timestamp": timestamp,
            "lesson_summary": lesson_summary.model_dump(),
            "skill_set": skill_set.model_dump() if skill_set else None,
            "num_groups": num_groups,
            "num_students": num_students,
            "teaching_packs": serialized_packs,
            "teaching_pack_output_file": str(teaching_pack_output_path),
            "evaluations": [
                {
                    "group_id": e["group_id"],
                    "group_name": e["group_name"],
                    "evaluation": e["evaluation"].model_dump()
                }
                for e in evaluations
            ]
        }, f, indent=2, ensure_ascii=False)

    print(f"\n Results saved to: {results_file}")

    # Print summary
    print("\n" + "=" * 80)
    print("EXPERIMENT SUMMARY")
    print("=" * 80)

    for eval_result in evaluations:
        eval_data = eval_result["evaluation"]
        print(f"\n {eval_result['group_name']}")
        print(f"   Content Accuracy:       {eval_data.accuracy_total:.2%}")
        print(f"   Concept Coverage:       {eval_data.coverage_total:.2%}")
        print(f"   Educational Soundness:  {eval_data.educational_soundness_total:.2%}")
        print(f"   Overall Score:          {eval_data.overall_score:.2%}")

    # Calculate average scores
    avg_accuracy = sum(e["evaluation"].accuracy_total for e in evaluations) / len(evaluations)
    avg_coverage = sum(e["evaluation"].coverage_total for e in evaluations) / len(evaluations)
    avg_soundness = sum(e["evaluation"].educational_soundness_total for e in evaluations) / len(evaluations)
    avg_overall = sum(e["evaluation"].overall_score for e in evaluations) / len(evaluations)

    print("\n" + "=" * 80)
    print("AVERAGE SCORES ACROSS ALL GROUPS")
    print("=" * 80)
    print(f"\n Avg Content Accuracy:       {avg_accuracy:.2%}")
    print(f" Avg Concept Coverage:       {avg_coverage:.2%}")
    print(f" Avg Educational Soundness:  {avg_soundness:.2%}")
    print(f"\n AVG OVERALL SCORE:          {avg_overall:.2%}")
    print("=" * 80)


# =====================================================
# CLI ENTRY POINT
# =====================================================

def main():
    parser = argparse.ArgumentParser(
        description="Run single-agent evaluation experiment with Gemini as judge"
    )
    parser.add_argument(
        "--config",
        type=str,
        default="config/default.yaml",
        help="Path to YAML config file (default: config/default.yaml)"
    )
    parser.add_argument(
        "--lesson_summary",
        type=str,
        required=True,
        help="Path to lesson summary JSON file or PDF lesson file"
    )
    parser.add_argument(
        "--ground_truth",
        type=str,
        default=None,
        help="Path to ground truth JSON file (optional)"
    )
    parser.add_argument(
        "--output_dir",
        type=str,
        default=None,
        help="Directory to save results (default: from config)"
    )
    parser.add_argument(
        "--num_groups",
        type=int,
        default=None,
        help="Number of student groups (default: from config)"
    )
    parser.add_argument(
        "--num_students",
        type=int,
        default=None,
        help="Total number of students (default: from config)"
    )
    parser.add_argument(
        "--seed",
        type=int,
        default=None,
        help="Random seed (default: from config)"
    )

    args = parser.parse_args()

    config = load_config(args.config)
    num_groups = resolve_value(args.num_groups, config, ["evaluation", "num_groups"], 3)
    num_students = resolve_value(args.num_students, config, ["evaluation", "num_students"], 30)
    output_dir = resolve_value(args.output_dir, config, ["paths", "output_dir"], "results/experiments")
    seed = resolve_value(args.seed, config, ["seed"], None)

    pipeline_model = get_config_value(config, ["models", "single_agent", "pipeline"], "gemini-2.0-flash")
    evaluator_model = get_config_value(config, ["models", "single_agent", "evaluator"], "gemini-2.5-flash")

    set_seed(seed)

    # Run experiment
    asyncio.run(run_experiment(
        lesson_summary_path=args.lesson_summary,
        ground_truth_path=args.ground_truth,
        output_dir=output_dir,
        num_groups=num_groups,
        num_students=num_students,
        pipeline_model=pipeline_model,
        evaluator_model=evaluator_model,
    ))


if __name__ == "__main__":
    main()
